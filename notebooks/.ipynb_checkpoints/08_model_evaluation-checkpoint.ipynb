{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Game Prediction - Model Evaluation and Interpretation\n",
    "\n",
    "This notebook focuses on evaluating and interpreting our best model. We'll:\n",
    "1. Load the best model\n",
    "2. Perform detailed evaluation\n",
    "3. Use SHAP values for model interpretation\n",
    "4. Analyze feature importance\n",
    "5. Generate insights and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "Import all necessary libraries for model evaluation and interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import shap\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve, average_precision_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Best Model\n",
    "Load the test data and the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load test data\n",
    "X_test = pd.read_csv('../data/processed/X_test_selected.csv')\n",
    "y_test = pd.read_csv('../data/processed/y_test.csv')\n",
    "\n",
    "# Load best model\n",
    "best_model = joblib.load('../models/best_model.joblib')\n",
    "\n",
    "print('Test set shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "Perform detailed evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score(y_test['TARGET'], y_pred),\n",
    "    'Precision': precision_score(y_test['TARGET'], y_pred),\n",
    "    'Recall': recall_score(y_test['TARGET'], y_pred),\n",
    "    'F1 Score': f1_score(y_test['TARGET'], y_pred),\n",
    "    'ROC AUC': roc_auc_score(y_test['TARGET'], y_pred_proba)\n",
    "}\n",
    "\n",
    "print('Model Performance Metrics:')\n",
    "for metric, value in metrics.items():\n",
    "    print(f'{metric}: {value:.3f}')\n",
    "\n",
    "# Print classification report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test['TARGET'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization of Model Performance\n",
    "Create visualizations to better understand model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test['TARGET'], y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test['TARGET'], y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {metrics[\"ROC AUC\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot precision-recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_test['TARGET'], y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'PR curve (AP = {average_precision_score(y_test[\"TARGET\"], y_pred_proba):.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Interpretation using SHAP\n",
    "Use SHAP values to understand feature importance and model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate SHAP values\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Plot summary plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_test, plot_type='bar')\n",
    "plt.title('Feature Importance (SHAP Values)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot detailed SHAP values\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "plt.title('SHAP Value Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis\n",
    "Analyze the importance of each feature in making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate mean absolute SHAP values\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_test.columns,\n",
    "    'importance': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Feature Importance (Mean |SHAP Value|)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('../data/processed/feature_importance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Insights and Recommendations\n",
    "Based on the model evaluation and interpretation, generate insights and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate feature correlations with predictions\n",
    "correlations = X_test.corrwith(pd.Series(y_pred_proba)).sort_values(ascending=False)\n",
    "\n",
    "print('Feature Correlations with Predictions:')\n",
    "print(correlations)\n",
    "\n",
    "# Save correlations\n",
    "pd.DataFrame({'feature': correlations.index, 'correlation': correlations.values}).to_csv(\n",
    "    '../data/processed/feature_correlations.csv', index=False\n",
    ")\n",
    "\n",
    "print('\\nInsights and recommendations have been saved to ../data/processed/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}